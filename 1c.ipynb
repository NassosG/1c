{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@misc{alpaca,\n",
    "  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },\n",
    "  title = {Stanford Alpaca: An Instruction-following LLaMA model},\n",
    "  year = {2023},\n",
    "  publisher = {GitHub},\n",
    "  journal = {GitHub repository},\n",
    "  howpublished = {\\url{https://github.com/tatsu-lab/stanford_alpaca}},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\auto\\auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97de4b97ff44c2c98dd763ab10231fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:732: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': ['input', 'output', 'instruction']}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b829e32ff734650962ffd41a6b85c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/37267 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e83e807dc84a01bb0ad23084e462cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4141 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f8755c67814e06bca50272c78c8440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10352 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'wandb.proto.wandb_internal_pb2' has no attribute 'Result'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mf:\\1c\\1c.ipynb Cell 2\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/1c/1c.ipynb#W0sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/1c/1c.ipynb#W0sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./results\u001b[39m\u001b[39m\"\u001b[39m,          \n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/1c/1c.ipynb#W0sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     num_train_epochs\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,              \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/1c/1c.ipynb#W0sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     load_best_model_at_end\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,     \n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/1c/1c.ipynb#W0sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/1c/1c.ipynb#W0sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# Initialize the Trainer with training and validation datasets\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/1c/1c.ipynb#W0sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/1c/1c.ipynb#W0sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,                               \n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/1c/1c.ipynb#W0sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     args\u001b[39m=\u001b[39;49mtraining_args,                        \n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/1c/1c.ipynb#W0sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39;49mtokenized_datasets[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m],               \n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/1c/1c.ipynb#W0sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39;49mtokenized_datasets[\u001b[39m'\u001b[39;49m\u001b[39mvalidation\u001b[39;49m\u001b[39m'\u001b[39;49m],                 \n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/1c/1c.ipynb#W0sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/1c/1c.ipynb#W0sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m# Training and validation\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/1c/1c.ipynb#W0sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    537\u001b[0m default_callbacks \u001b[39m=\u001b[39m DEFAULT_CALLBACKS \u001b[39m+\u001b[39m get_reporting_integration_callbacks(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mreport_to)\n\u001b[0;32m    538\u001b[0m callbacks \u001b[39m=\u001b[39m default_callbacks \u001b[39mif\u001b[39;00m callbacks \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m default_callbacks \u001b[39m+\u001b[39m callbacks\n\u001b[1;32m--> 539\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler \u001b[39m=\u001b[39m CallbackHandler(\n\u001b[0;32m    540\u001b[0m     callbacks, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlr_scheduler\n\u001b[0;32m    541\u001b[0m )\n\u001b[0;32m    542\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_callback(PrinterCallback \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdisable_tqdm \u001b[39melse\u001b[39;00m DEFAULT_PROGRESS_CALLBACK)\n\u001b[0;32m    544\u001b[0m \u001b[39m# Will be set to True by `self._setup_loggers()` on first call to `self.log()`.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\trainer_callback.py:313\u001b[0m, in \u001b[0;36mCallbackHandler.__init__\u001b[1;34m(self, callbacks, model, tokenizer, optimizer, lr_scheduler)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks \u001b[39m=\u001b[39m []\n\u001b[0;32m    312\u001b[0m \u001b[39mfor\u001b[39;00m cb \u001b[39min\u001b[39;00m callbacks:\n\u001b[1;32m--> 313\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_callback(cb)\n\u001b[0;32m    314\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[0;32m    315\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m tokenizer\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\trainer_callback.py:330\u001b[0m, in \u001b[0;36mCallbackHandler.add_callback\u001b[1;34m(self, callback)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd_callback\u001b[39m(\u001b[39mself\u001b[39m, callback):\n\u001b[1;32m--> 330\u001b[0m     cb \u001b[39m=\u001b[39m callback() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(callback, \u001b[39mtype\u001b[39m) \u001b[39melse\u001b[39;00m callback\n\u001b[0;32m    331\u001b[0m     cb_class \u001b[39m=\u001b[39m callback \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(callback, \u001b[39mtype\u001b[39m) \u001b[39melse\u001b[39;00m callback\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\n\u001b[0;32m    332\u001b[0m     \u001b[39mif\u001b[39;00m cb_class \u001b[39min\u001b[39;00m [c\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks]:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\integrations\\integration_utils.py:676\u001b[0m, in \u001b[0;36mWandbCallback.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    674\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mWandbCallback requires wandb to be installed. Run `pip install wandb`.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    675\u001b[0m \u001b[39mif\u001b[39;00m has_wandb:\n\u001b[1;32m--> 676\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mwandb\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wandb \u001b[39m=\u001b[39m wandb\n\u001b[0;32m    679\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initialized \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\wandb\\__init__.py:27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m# This needs to be early as other modules call it.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwandb\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39merrors\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mterm\u001b[39;00m \u001b[39mimport\u001b[39;00m termsetup, termlog, termerror, termwarn\n\u001b[1;32m---> 27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwandb\u001b[39;00m \u001b[39mimport\u001b[39;00m sdk \u001b[39mas\u001b[39;00m wandb_sdk\n\u001b[0;32m     29\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwandb\u001b[39;00m\n\u001b[0;32m     31\u001b[0m wandb\u001b[39m.\u001b[39mwandb_lib \u001b[39m=\u001b[39m wandb_sdk\u001b[39m.\u001b[39mlib  \u001b[39m# type: ignore\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\wandb\\sdk\\__init__.py:25\u001b[0m\n\u001b[0;32m      3\u001b[0m __all__ \u001b[39m=\u001b[39m (\n\u001b[0;32m      4\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mConfig\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mSettings\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mhelper\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m wandb_helper \u001b[39mas\u001b[39;00m helper\n\u001b[1;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39martifacts\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39martifact\u001b[39;00m \u001b[39mimport\u001b[39;00m Artifact\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mwandb_alerts\u001b[39;00m \u001b[39mimport\u001b[39;00m AlertLevel\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mwandb_config\u001b[39;00m \u001b[39mimport\u001b[39;00m Config\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\wandb\\sdk\\artifacts\\artifact.py:37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwandb\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwandb\u001b[39;00m \u001b[39mimport\u001b[39;00m data_types, env, util\n\u001b[1;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwandb\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapis\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnormalize\u001b[39;00m \u001b[39mimport\u001b[39;00m normalize_exceptions\n\u001b[0;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwandb\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapis\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpublic\u001b[39;00m \u001b[39mimport\u001b[39;00m ArtifactCollection, ArtifactFiles, RetryingClient, Run\n\u001b[0;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwandb\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_types\u001b[39;00m \u001b[39mimport\u001b[39;00m WBValue\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\wandb\\apis\\__init__.py:43\u001b[0m\n\u001b[0;32m     38\u001b[0m     _disable_ssl()\n\u001b[0;32m     41\u001b[0m reset_path \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mvendor_setup()\n\u001b[1;32m---> 43\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39minternal\u001b[39;00m \u001b[39mimport\u001b[39;00m Api \u001b[39mas\u001b[39;00m InternalApi  \u001b[39m# noqa\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpublic\u001b[39;00m \u001b[39mimport\u001b[39;00m Api \u001b[39mas\u001b[39;00m PublicApi  \u001b[39m# noqa\u001b[39;00m\n\u001b[0;32m     46\u001b[0m reset_path()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\wandb\\apis\\internal.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwandb\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msdk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternal\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternal_api\u001b[39;00m \u001b[39mimport\u001b[39;00m Api \u001b[39mas\u001b[39;00m InternalApi\n\u001b[0;32m      6\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mApi\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Internal proxy to the official internal API.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\wandb\\sdk\\internal\\internal_api.py:48\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwandb\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msdk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgql_request\u001b[39;00m \u001b[39mimport\u001b[39;00m GraphQLSession\n\u001b[0;32m     46\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwandb\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msdk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhashutil\u001b[39;00m \u001b[39mimport\u001b[39;00m B64MD5, md5_file_b64\n\u001b[1;32m---> 48\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlib\u001b[39;00m \u001b[39mimport\u001b[39;00m retry\n\u001b[0;32m     49\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfilenames\u001b[39;00m \u001b[39mimport\u001b[39;00m DIFF_FNAME, METADATA_FNAME\n\u001b[0;32m     50\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgitlib\u001b[39;00m \u001b[39mimport\u001b[39;00m GitRepo\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\wandb\\sdk\\lib\\retry.py:17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwandb\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwandb\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m CheckRetryFnType\n\u001b[1;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmailbox\u001b[39;00m \u001b[39mimport\u001b[39;00m ContextCancelledError\n\u001b[0;32m     19\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[39m# To let tests mock out the retry logic's now()/sleep() funcs, this file\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m# should only use these variables, not call the stdlib funcs directly.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\wandb\\sdk\\lib\\mailbox.py:102\u001b[0m\n\u001b[0;32m     98\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event\u001b[39m.\u001b[39mclear()\n\u001b[0;32m     99\u001b[0m         \u001b[39mreturn\u001b[39;00m found\n\u001b[1;32m--> 102\u001b[0m \u001b[39mclass\u001b[39;49;00m \u001b[39m_MailboxSlot\u001b[39;49;00m:\n\u001b[0;32m    103\u001b[0m     _result: Optional[pb\u001b[39m.\u001b[39;49mResult]\n\u001b[0;32m    104\u001b[0m     _event: threading\u001b[39m.\u001b[39;49mEvent\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\wandb\\sdk\\lib\\mailbox.py:103\u001b[0m, in \u001b[0;36m_MailboxSlot\u001b[1;34m()\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39m_MailboxSlot\u001b[39;00m:\n\u001b[1;32m--> 103\u001b[0m     _result: Optional[pb\u001b[39m.\u001b[39;49mResult]\n\u001b[0;32m    104\u001b[0m     _event: threading\u001b[39m.\u001b[39mEvent\n\u001b[0;32m    105\u001b[0m     _lock: threading\u001b[39m.\u001b[39mLock\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'wandb.proto.wandb_internal_pb2' has no attribute 'Result'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, BitsAndBytesConfig\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import pipeline\n",
    "\n",
    "# Model configuration\n",
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "token = \"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\"  \n",
    "\n",
    "# Load model and tokenizer without quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=token)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, use_auth_token=token)\n",
    "\n",
    "\n",
    "# Load and prepare the dataset\n",
    "full_dataset = load_dataset(\"yahma/alpaca-cleaned\")\n",
    "split_datasets = full_dataset['train'].train_test_split(test_size=0.2)  # Splitting 20% for testing\n",
    "train_val_split = split_datasets['train'].train_test_split(test_size=0.1)  # Further splitting the training set for validation\n",
    "\n",
    "# Wrap splits in a DatasetDict for convenience\n",
    "dataset_splits = DatasetDict({\n",
    "    'train': train_val_split['train'],\n",
    "    'validation': train_val_split['test'],\n",
    "    'test': split_datasets['test']\n",
    "})\n",
    "\n",
    "# Print the column names of the dataset\n",
    "print(full_dataset.column_names)\n",
    "\n",
    "# Tokenize the datasets\n",
    "def tokenize_function(examples):\n",
    "    concatenated_texts = [instr + \" [SEP] \" + inp for instr, inp in zip(examples[\"instruction\"], examples[\"input\"])]\n",
    "    return tokenizer(concatenated_texts, padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset_splits.map(tokenize_function, batched=True)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          \n",
    "    num_train_epochs=3,              \n",
    "    per_device_train_batch_size=4,   \n",
    "    per_device_eval_batch_size=8,    \n",
    "    warmup_steps=500,                \n",
    "    weight_decay=0.01,               \n",
    "    logging_dir=\"./logs\",            \n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",     \n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,     \n",
    ")\n",
    "\n",
    "# Initialize the Trainer with training and validation datasets\n",
    "trainer = Trainer(\n",
    "    model=model,                               \n",
    "    args=training_args,                        \n",
    "    train_dataset=tokenized_datasets['train'],               \n",
    "    eval_dataset=tokenized_datasets['validation'],                 \n",
    ")\n",
    "\n",
    "# Training and validation\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model using the Trainer on the test set\n",
    "test_results = trainer.predict(tokenized_datasets['test'])\n",
    "print(\"Test Results:\", test_results.metrics)\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "generation_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Now generate predictions as before\n",
    "test_texts = [tokenizer.decode(g, skip_special_tokens=True) for g in tokenized_datasets[\"test\"][\"input_ids\"][:20]]  # Assuming you want to generate for the first 20 examples for demonstration\n",
    "\n",
    "predictions = [\n",
    "    generation_pipeline(\n",
    "        text,\n",
    "        max_length=50,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,\n",
    "        temperature=0.9\n",
    "    )[0]['generated_text'] for text in test_texts\n",
    "]\n",
    "\n",
    "# Printing a few predictions for demonstration\n",
    "for i, prediction in enumerate(predictions[:5]):\n",
    "    print(f\"Prediction {i+1}: {prediction}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
